{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.utils import crawl_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn as sk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score ,f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = \"annotations/label\"\n",
    "video_features = \"DATA/visual_features\"\n",
    "audio_features = \"DATA/audio_features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling data paths\n"
     ]
    }
   ],
   "source": [
    "print(\"Crawling data paths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tree = crawl_directory(labels_path)\n",
    "videos_tree = crawl_directory(video_features)\n",
    "audio_tree = crawl_directory(audio_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of features and annotations\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Number of features and annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels len:  418\n",
      "Audio len:  418\n",
      "Video len:  418\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels len: \", len(labels_tree))\n",
    "print(\"Audio len: \", len(audio_tree))\n",
    "print(\"Video len: \", len(videos_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting both lists with paths, in order to be in parallel\n"
     ]
    }
   ],
   "source": [
    "print(\"Sorting both lists with paths, in order to be in parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting files list in order to be in parallel\n",
    "\n",
    "labels_tree.sort()\n",
    "videos_tree.sort()\n",
    "audio_tree.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels_tree) == len(videos_tree) == len(audio_tree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_files(tree: list):\n",
    "    \"\"\"\n",
    "    Getting the unique file names from a list\n",
    "    \"\"\"\n",
    "    sunolo = set()\n",
    "    for i in tree:\n",
    "        sunolo.add(i.split(os.sep)[-1].split('.')[0])\n",
    "    return sunolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npys_to_matrics(labels: list, videos: list, audio: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Loading the numpy files. Visual and audio will be averaged every 5 and 10 rows respectively. \n",
    "    DISCLAIMER i keep the minimum number of samples between the same video file from label, video and audio features matrices.\n",
    "    \"\"\"\n",
    "    print(\"Nunpy to Matrices have start\")\n",
    "    files_sizes = []\n",
    "    labels_matrix = []\n",
    "    visual_matrix = []\n",
    "    audio_matrix = []\n",
    "    if not len(labels) == len(videos) == len(audio):\n",
    "        raise Exception(\"Labels, visual features and audio have not the same size\")\n",
    "    for idx in range(len(labels)):\n",
    "        \n",
    "      \n",
    "        # load labels, visual and audio in temporary variables\n",
    "        try:\n",
    "            \n",
    "            tmp_label = np.load(labels[idx])\n",
    "            tmp_visual = np.load(videos[idx])\n",
    "            tmp_audio = np.load(audio[idx]).transpose() # transposed to the same format of visual features (rows = samplles, columns = features)\n",
    "        except ValueError:\n",
    "            print(f'File in index {idx} with name {videos[idx]} Not loaded')\n",
    "            continue\n",
    "        \n",
    "        # get min seconds from the same label, visual, audio np file \n",
    "        l_r = tmp_label.shape[0]\n",
    "        v_r, v_c = tmp_visual.shape\n",
    "        a_r, a_c = tmp_audio.shape\n",
    "        \n",
    "        v_r = v_r//5\n",
    "        a_r = a_r//10\n",
    "        min_seconds = min(l_r, v_r, a_r)\n",
    "\n",
    "        files_sizes.append(min_seconds)\n",
    "        \n",
    "        labels_matrix.append(tmp_label[:min_seconds])\n",
    "        # VISUAL\n",
    "        # keep number of samples divisible with 5\n",
    "        tmp_visual = tmp_visual[:min_seconds*5]\n",
    "        # averaging visual every 5 (Because we have analyze video with .2 step)\n",
    "        visual_matrix.append(tmp_visual.transpose().reshape(-1,5).mean(1).reshape(v_c,-1).transpose())\n",
    "        \n",
    "        # AUDIO\n",
    "        # keep number of samples divisible with 10\n",
    "        tmp_audio = tmp_audio[:min_seconds*10]\n",
    "        # averaging audio every 10 (Because we have analyze video with .1 step)\n",
    "        audio_matrix.append(tmp_audio.transpose().reshape(-1,10).mean(1).reshape(a_c,-1).transpose())\n",
    "        \n",
    "        \n",
    "        del tmp_label\n",
    "        del tmp_visual\n",
    "        del tmp_audio\n",
    "        \n",
    "    return files_sizes, labels_matrix, visual_matrix, audio_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(labels: list, videos: list, audio: list, split_size: float) -> tuple:\n",
    "    if not len(labels) == len(videos) == len(audio):\n",
    "        raise Exception(\"Labels, visual features and audio have not the same size\")\n",
    "    if split_size >= 1.0 or split_size <= 0.0:\n",
    "        raise Exception(\"Split size is out of bound\")\n",
    "    trainining_size = int(split_size * len(labels))\n",
    "    #first training, second test\n",
    "    return np.hstack([label for label in labels[:trainining_size]]),np.vstack([video for video in videos[:trainining_size]]), np.vstack([audio for audio in audio[:trainining_size]]), \\\n",
    "           np.hstack([label for label in labels[trainining_size:]]), np.vstack([video for video in videos[trainining_size:]]), np.vstack([audio for audio in audio[trainining_size:]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auto den paizei \n",
    "# video_72 = np.load(videos_tree[72])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nunpy to Matrices have start\n",
      "File in index 72 with name DATA/visual_features/downhill/Matt Jones - DOWNHILL BRAKELESS BMX IS DANGEROUS!!.mp4.npy Not loaded\n"
     ]
    }
   ],
   "source": [
    "files_sizes, labels_matrix, visual_matrix, audio_matrix = load_npys_to_matrics(labels_tree, videos_tree, audio_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_visual, train_audio, test_labels, test_visual, test_audio = split(labels_matrix, visual_matrix, audio_matrix, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SHAPES\n",
      "Labels Shape (165437,)\n",
      "Visual Shape (165437, 88)\n",
      "Audio Shape (165437, 68)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN SHAPES\")\n",
    "print(f'Labels Shape {train_labels.shape}')\n",
    "print(f'Visual Shape {train_visual.shape}')\n",
    "print(f'Audio Shape {train_audio.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST SHAPES\n",
      "Labels Shape (39540,)\n",
      "Visual Shape (39540, 88)\n",
      "Audio Shape (39540, 68)\n"
     ]
    }
   ],
   "source": [
    "print(\"TEST SHAPES\")\n",
    "print(f'Labels Shape {test_labels.shape}')\n",
    "print(f'Visual Shape {test_visual.shape}')\n",
    "print(f'Audio Shape {test_audio.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisrt Try Every number grater than 1, will be 1. Making binary classification (dummy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_one_labels = train_labels.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_one_labels[zero_one_labels>1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165437,)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_one_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped = zero_one_labels.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165437, 1)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "lR = LogisticRegression(solver='lbfgs', max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 26s, sys: 1min 35s, total: 5min 2s\n",
      "Wall time: 38.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=10000)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lR.fit(train_visual, zero_one_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lR.predict(test_visual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(test_labels, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(test_labels, predicted, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(test_labels, predicted, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(test_labels, predicted, average='macro', zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.4853313100657562\n",
      "f1 0.14350974680376488\n",
      "Recall 0.15378507378043757\n",
      "Precision 0.13683928975566778\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy {acc}')\n",
    "print(f'f1 {f1}')\n",
    "print(f'Recall { recall}')\n",
    "print(f'Precision {precision}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}